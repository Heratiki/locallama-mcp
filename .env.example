# Server Configuration: These settings configure the server's basic networking.
PORT=3000
HOST=0.0.0.0
API_PREFIX=/api

# Local LLM Endpoints: These settings configure the endpoints for local Large Language Models.
LM_STUDIO_ENDPOINT=http://localhost:1234/v1
OLLAMA_ENDPOINT=http://localhost:11434/api

# Model Configuration: These settings configure the behavior of the language model.
DEFAULT_LOCAL_MODEL=llama2
MODEL_TEMPERATURE=0.1
MODEL_MAX_TOKENS=2048
MODEL_TOP_P=0.95
MODEL_FREQUENCY_PENALTY=0
MODEL_PRESENCE_PENALTY=0

# Decision Thresholds: These settings determine when to split tasks, control costs, and ensure quality.
TOKEN_THRESHOLD=1000
COST_THRESHOLD=0.02
QUALITY_THRESHOLD=0.7

# API Keys: These settings configure API keys for external services.
OPENROUTER_API_KEY=

# Benchmark Configuration: These settings control how benchmarks are run.
BENCHMARK_RUNS_PER_TASK=3
BENCHMARK_PARALLEL=false
BENCHMARK_MAX_PARALLEL_TASKS=2
BENCHMARK_TASK_TIMEOUT=60000
BENCHMARK_SAVE_RESULTS=true
BENCHMARK_RESULTS_PATH=./benchmark-results

# Logging Configuration: These settings configure the logging behavior of the application.
LOG_LEVEL=info
LOG_FILE=

# Python Configuration: These settings configure the Python environment.
PYTHON_PATH=
PYTHON_VENV_PATH=
PYTHON_DETECT_VENV=true
RETRIV_PYTHON_PATH=

# Code Search Configuration: These settings configure the semantic code search functionality.
CODE_SEARCH_ENABLED=true
CODE_SEARCH_EXCLUDE_PATTERNS=["node_modules/**","dist/**",".git/**"]
CODE_SEARCH_INDEX_ON_START=true
CODE_SEARCH_REINDEX_INTERVAL=3600

# Cache Settings: These settings configure the response caching mechanism.
CACHE_ENABLED=true
CACHE_DIR=./.cache
MAX_CACHE_SIZE=1073741824
